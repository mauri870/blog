---
title: "The Low-Background Web"
date: 2025-04-06T19:15:00-03:00
tags: ["AI", "LLM", "Web", "Philosophy"]
draft: false
---

We are generating more content than ever — but how much of it is still truly human? And how do we even know what's real anymore? Let's talk about the Low-Background Web.

<!--more-->

---

## 1. What's low-background anyway?

If you've ever read about nuclear physics or worked with sensitive instrumentation, you might've stumbled upon the term *low-background steel*. According to the [Wikipedia article](https://en.wikipedia.org/wiki/Low-background_steel) on the topic, it can be summarized as:

> Any steel produced prior to the detonation of the first nuclear bombs in the 1940s and 1950s. Typically sourced from ships and other steel artifacts of this era, it is often used for modern particle detectors because more modern steel is contaminated with traces of nuclear fallout.

Now think of that, but on the context of the web and AI-generated content.

## 2. Before the storm

I'm calling it the **low-background web**: all the internet content that came *before* the rise of large-scale generative models. That's blogs, wikis, forums, documentation, and general-purpose content written by humans for humans, even before the advent of the internet itself.

Back then the web was slower, rougher, and let's be honest, full of garbage. But at least it was *our original* garbage. Not content regurgitated by AI.

We are living in a time where it is very difficult to trust the source of information, and it tends to get worse as the years go by. At some point, it will be basically impossible to distinguish what's real using human senses alone.

After the advent of large language models (LLMs) around 2018, the web started to get flooded with AI-generated content. Articles, blog posts, images and video — you name it. This is not just a problem of quality, but also of authenticity. It's becoming increasingly difficult to discern what's real.

Besides the philosophical  implications, there is also the feedback loop problem. LLMs are now trained on content from previous LLMs, which were trained on earlier LLMs... and so on. It's like photocopying a drawing a dozen times in a row. It still packs information, but at some point, the signal gets lost in the noise.

Imagine searching for a recipe for a meal you plan to cook. You find lots of posts, half a dozen are AI generated with more or less the same instructions, regurgitating the same information. No one comes up with new ideas, no one tries to innovate, we keep iterating on the same thing over and over. This can result in another phenomenon I call *the ice aging of information*.

Humans are naturally creative and curious, curiosity has been the force that drives innovation and improvement since the beginning of our species. When we lose that spark, creativity, innovation and progress tend to stagnate.

The low-background is the signal. Don't follow the white rabbit.

## 3. A moving window

The tricky part? The boundary keeps shifting. As LLM-generated content gets indexed, reposted, repackaged, and redistributed, it gets harder to know what's clean and what's not. There's no metadata that says "hey, this paragraph was written 100% by a human.". Even if there was, would you still trust it?

Today's web is noisy, the low-background content is getting harder and harder to find.

So if you're browsing a blog, watching a video, or just into the archaeology of internet culture, maybe start thinking in terms of *low-background*. You'll know it when you see it. 

---

That is it. The Low-background web. Let's not lose track of where we came from.

See ya next time, unless I've been replaced by an AI. 
Hmmm, maybe I already have. Who knows?

> Knock, knock, Neo.
